---
title: "Dunlop - death estimation retry"
author: "Cassandra Wattenburger"
date: "12/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "show")
knitr::opts_chunk$set(message = FALSE)
```

```{r}
rm(list=ls())

library("tidyverse")

sessionInfo()
```

Applying success from recent SFA2 experiment here to see if I can estimate death in this dataset.

# Import data

Normalized abundances, estimated growth.

```{r}
# Normalized abundance data
norm <- readRDS("../rdata.files/gr_ucosm.norm.clean.rds")

# Growth estimates
growth_est <- readRDS("../rdata.files/gr_gr.paprica.clean.rds")
```

# Prepare normalized abundance data for death estimation

Same steps as 03_dunlopgr_grestimation.Rmd.

```{r}
# Remove 0s
norm_zerorm <- filter(norm, norm_abund > 0)

# Remove time series with < 3 tps remaining
occurences <- norm_zerorm %>% 
  group_by(Soil, Amendment, Replicate, ASV) %>% 
  summarize(occurs = n()) %>% 
  filter(occurs > 2) %>% 
  ungroup()

norm_3tp <- inner_join(norm_zerorm, occurences) %>% 
  select(everything(), -occurs)

# Natural log transform
norm_ln <- mutate(norm_3tp, abund_ln = log(norm_abund))

# Create unique labels for each time series
norm_ln <- mutate(norm_ln, Label = paste0(Soil, Amendment, Replicate, ASV))
```

# Fit linear model to time series

Same method as 03_dunlopgr_grestimation.Rmd but looking for slope < 0 instead.

```{r}
# Save estimate function
savefit <- function(start, end, datasub, output) { # Start point, end point, time series data, output dataframe
  est <- NULL; coeff <- NULL; resids <- NULL; pval <- NULL; thisrow <- data.frame() # clear previous
  est <- lm(abund_ln ~ Day, data=datasub[start:end,])
  coeff <- est$coefficients[2]
  resids <- sum(abs(resid(est)))
  pval <- summary(est)$coefficients[2,4]
  thisrow <- bind_cols(Label=label, Soil=soil, Amendment=amend, Replicate=rep, ASV=asv, 
                       start=start, end=end, coeff=coeff, pval=pval, residuals=resids, row.names <- NULL) # breaking here: arguments imply differing number of rows: 1, 0
  output <- bind_rows(output, thisrow)
  return(output)
}

# Fit linear model with sliding window
death_est <- data.frame()
for (label in as.character(unique(norm_ln$Label))) {
  # Subset one time series
  datasub <- data.frame()
  datasub <- norm_ln[norm_ln$Label==label,] # subset growth curve
  stop <- FALSE
  # Save time series info
  label <- NULL; soil <- NULL; amend = NULL; rep = NULL; asv = NULL
  label <- as.character(unique(datasub$Label))
  soil <- as.character(unique(datasub$Soil))
  amend <- as.character(unique(datasub$Amendment))
  rep <- as.numeric(unique(datasub$Replicate))
  asv <- as.character(unique(datasub$ASV))
  
  # Sliding window
  for (b in 1:(nrow(datasub)-2)) { # start of window
    #if (stop == TRUE) {break} this would stop the loop after the first solution is found, but we decided to consider all windows instead
    start <- b
    stop <- FALSE # this ensures all windows are considered (for each new b/window, stop is reset to false)
    for (e in (b+2):nrow(datasub)) { # end of window
      if (stop == TRUE) {break} # stop extending the window if a solution is found
   
      # Fit linear model to window
      test_lm <- NULL; test_pval <- NULL; test_coeff <- NULL
      test_lm <- lm(abund_ln ~ Day, data=datasub[b:e,])
      test_pval <- summary(test_lm)$coefficients[2,4]
      test_coeff <- test_lm$coefficients[2]

      # Extend the window
      if (test_pval <= 0.05 & test_coeff < 0 & e < nrow(datasub)) { # good fit, but more data might improve?
        for (x in ((e+1):nrow(datasub))) { # extend this window to see if it improves
          if (stop == TRUE) {break} # stop extending if the solution improves or if hit end of time series
          testprev_lm <- NULL; testprev_pval <- NULL; testprev_coeff <- NULL; testnew_lm <- NULL; testnew_pval <- NULL; testnew_coeff <- NULL
          testprev_lm <- lm(abund_ln ~ Day, data=datasub[b:(x-1),])
          testprev_pval <- summary(testprev_lm)$coefficients[2,4]
          testprev_coeff <- testprev_lm$coefficients[2]
          testnew_lm <- lm(abund_ln ~ Day, data=datasub[b:x,])
          testnew_pval <- summary(testnew_lm)$coefficients[2,4]
          testnew_coeff <- testnew_lm$coefficients[2]
          
          # Continue extending the window
          if (testnew_pval <= testprev_pval & testnew_coeff < 0 & x < nrow(datasub)) { # if adding data improved (or didn't hurt) the estimate, keep extending
            next
          }
          
          # No improvement
          else if (testnew_pval > testprev_pval & testprev_coeff < 0 & x < nrow(datasub)) { # if adding data did not improve the estimate, stop
            end <- x-1
            death_est <- savefit(start, end, datasub, death_est) # where it tried to save but failed, think issue is in savefit function
            stop <- TRUE
          }
          
          # No more data pts to fit
          else if (testnew_pval <= testprev_pval & testnew_coeff < 0 & x == nrow(datasub)) { # improved but no more data
            end <- x
            death_est <- savefit(start, end, datasub, death_est)
            stop <- TRUE
          } 
        }
      }
      
      # No more data pts to fit
      else if (test_pval <= 0.05 & test_coeff < 0 & e==nrow(datasub)) { # meets threshold but no remaining points to extend
        end <- e
        death_est <- savefit(start, end, datasub, death_est)
        stop <- TRUE
      }
    }
  }
}
```

# Remove "essentially perfect fits"

I can't find any guidance on how to determine whether or not a fit is "perfect" but I know that the residuals are essentially equally to 0 for perfect fits. I'll use 0.0001 as a filtering threshold for removal.

```{r}
# Remove "perfect" fits as precaution
death_noperf <- filter(death_est, residuals >= 0.0001)
nrow(death_est)
nrow(death_noperf)
```

# Select best fit for each time series

* Smallest slope p-value

```{r}
# Best p-value for each curve
death_lowestp <- death_noperf %>% 
  group_by(Label) %>% 
  summarize(pval = min(pval)) %>% 
  ungroup()

death_best <- semi_join(death_noperf, death_lowestp)

nrow(death_best)
```


# False positive control

Histogram of quality filtered p-values from actual estimates:

```{r}
hist(death_best$pval, xlab="P-values", main="Histogram of quality filtered p-values")
```

This p-value distribution is anti-conservative based on the trailing right tail (this is a good thing). However, it appears that there must be some proportion of false positives based on how high that right tail is compared to the initial spike at the beginning of the distribution. We must do some form of multiple comparison control.

See: http://varianceexplained.org/statistics/interpreting-pvalue-histogram/

I'm choosing to use a permutation approach where I use my growth estimating algorhithm on randomly generated data with characteristics of the real data. I'll use this false positive information to filter my real estimates. Traditional false positive control methods are far too conservative for my dataset.

### Simulate random data

Completely random data designed to reflect actual data, if we detect "significant" growth rate estimates from this, we must control for that, because the same thing can happen in our actual data.

Information about real dataset to use for simulating random data:
* number of time points
* min and max of abundance

```{r, results="show"}
# Minimum and maximum relational abund_lnances
min_abund_ln <- min(norm_ln$abund_ln)
max_abund_ln <- max(norm_ln$abund_ln)
avg_abund_ln <- mean(norm_ln$abund_ln)
sd_abund_ln <- sd(norm_ln$abund_ln)

hist(norm_ln$abund_ln)

# Number of time points
num_tps <- norm_ln %>% 
  group_by(Label) %>% 
  summarize(num_points = n()) %>% 
  ungroup()

min_pts <- min(num_tps$num_points)
max_pts <- max(num_tps$num_points)
avg_pts <- mean(num_tps$num_points)
sd_pts <- sd(num_tps$num_points)

hist(num_tps$num_points)
```

Simulate random time series:

```{r}
set.seed(2021)

# Set time points
tps <- unique(norm$Day)
tps <- na.omit(tps)

# Generate using normal distributions
sim_data <- data.frame()
cont <-  TRUE
counter <- 1
while (cont == TRUE) {
  # Stop loop once 1000 simulations have been created
  if (counter == 1001) {
    cont <-  FALSE
  } 
  else {
    thisrow <- data.frame(); hold <- NULL; rand_abund_ln = NULL; rand_pts = NULL; rand_days = NULL # reset values
    # Generate number of time points
    rand_pts <- ceiling(rnorm(1, mean=avg_pts, sd=sd_pts))
    # Make sure number of tps doesn't fall outside min/max
    if (rand_pts <= min_pts | rand_pts >= max_pts) {
      next
      }
    else {
      # Generate abundance for each point
      rand_abund_ln <- rnorm(rand_pts, mean=avg_abund_ln, sd=sd_abund_ln) 
      # Make sure abundances don't fall outside min/max
      if (min(rand_abund_ln) <= min_abund_ln | max(rand_abund_ln) >= max_abund_ln) {
        next
      }
      else {
        # Assign a day to each point
        rand_day <- sort(sample(tps, size=rand_pts, replace=FALSE))
        hold <- cbind(rep(counter, rand_pts), rand_day, rand_abund_ln)
        sim_data= rbind(sim_data, hold)
        counter <- counter + 1
      }
    }
  }
}
colnames(sim_data)[1] <- "simulation"

# Compare simulation to actual data
# Abundances
hist(sim_data$rand_abund_ln)

# Number of data points
sim_num_points <- sim_data %>% 
  group_by(simulation) %>% 
  summarize(num_pts = n()) %>% 
  ungroup() %>% 
  mutate(num_pts = as.numeric(num_pts))
hist(sim_num_points$num_pts)
```

Look into how to make number of points simulated more skewed.

Save simulated data:

* for reproducibility

```{r, eval=FALSE}
saveRDS(sim_data, file="../rdata.files/gr_deathsimulation.rds")
```

View some simulated time series:

```{r}
# Choose randomly
sim_data %>%
  filter(simulation == sample(1:1000, 1)) %>% 
  ggplot(aes(x=rand_day, y=rand_abund_ln)) +
    geom_point() +
    geom_line() +
    theme_test()

sim_data %>%
  filter(simulation == sample(1:1000, 1)) %>% 
  ggplot(aes(x=rand_day, y=rand_abund_ln)) +
    geom_point() +
    geom_line() +
    theme_test()

sim_data %>%
  filter(simulation == sample(1:1000, 1)) %>% 
  ggplot(aes(x=rand_day, y=rand_abund_ln)) +
    geom_point() +
    geom_line() +
    theme_test()

sim_data %>%
  filter(simulation == sample(1:1000, 1)) %>% 
  ggplot(aes(x=rand_day, y=rand_abund_ln)) +
    geom_point() +
    geom_line() +
    theme_test()
```

### Estimate "death" on simulated data

```{r}
# Save estimate function
savefit <- function(start, end, datasub, output) { # Start point, end point, time series data, output dataframe
  est <- NULL; coeff <- NULL; resids <- NULL; pval <- NULL; thisrow <- data.frame() # clear previous
  est <- lm(rand_abund_ln ~ rand_day, data=datasub[start:end,])
  coeff <- est$coefficients[2]
  resids <- sum(abs(resid(est)))
  pval <- summary(est)$coefficients[2,4]
  thisrow <- bind_cols(simulation=s, start=start, end=end, coeff=coeff, pval=pval, residuals=resids, row.names <- NULL) # breaking here: arguments imply differing number of rows: 1, 0
  output <- bind_rows(output, thisrow)
  return(output)
}

# Fit linear model with sliding window
sim_est <- data.frame()
for (s in as.character(unique(sim_data$simulation))) {
  # Subset one time series
  datasub <- data.frame()
  datasub <- sim_data[sim_data$simulation==s,] # subset growth curve
  stop <- FALSE
  
  # Sliding window
  for (b in 1:(nrow(datasub)-2)) { # start of window
    #if (stop == TRUE) {break} this would stop the loop after the first solution is found, but we decided to consider all windows instead
    start <- b
    stop <- FALSE # this ensures all windows are considered (for each new b/window, stop is reset to false)
    for (e in (b+2):nrow(datasub)) { # end of window
      if (stop == TRUE) {break} # stop extending the window if a solution is found
   
      # Fit linear model to window
      test_lm <- NULL; test_pval <- NULL; test_coeff <- NULL
      test_lm <- lm(rand_abund_ln ~ rand_day, data=datasub[b:e,])
      test_pval <- summary(test_lm)$coefficients[2,4]
      test_coeff <- test_lm$coefficients[2]

      # Extend the window
      if (test_pval <= 0.05 & test_coeff < 0 & e < nrow(datasub)) { # good fit, but more data might improve?
        for (x in ((e+1):nrow(datasub))) { # extend this window to see if it improves
          if (stop == TRUE) {break} # stop extending if the solution improves or if hit end of time series
          testprev_lm <- NULL; testprev_pval <- NULL; testprev_coeff <- NULL; testnew_lm <- NULL; testnew_pval <- NULL; testnew_coeff <- NULL
          testprev_lm <- lm(rand_abund_ln ~ rand_day, data=datasub[b:(x-1),])
          testprev_pval <- summary(testprev_lm)$coefficients[2,4]
          testprev_coeff <- testprev_lm$coefficients[2]
          testnew_lm <- lm(rand_abund_ln ~ rand_day, data=datasub[b:x,])
          testnew_pval <- summary(testnew_lm)$coefficients[2,4]
          testnew_coeff <- testnew_lm$coefficients[2]
          
          # Continue extending the window
          if (testnew_pval <= testprev_pval & testnew_coeff < 0 & x < nrow(datasub)) { # if adding data improved (or didn't hurt) the estimate, keep extending
            next
          }
          
          # No improvement
          else if (testnew_pval > testprev_pval & testprev_coeff < 0 & x < nrow(datasub)) { # if adding data did not improve the estimate, stop
            end <- x-1
            sim_est <- savefit(start, end, datasub, sim_est) # where it tried to save but failed, think issue is in savefit function
            stop <- TRUE
          }
          
          # No more data pts to fit
          else if (testnew_pval <= testprev_pval & testnew_coeff < 0 & x == nrow(datasub)) { # improved but no more data
            end <- x
            sim_est <- savefit(start, end, datasub, sim_est)
            stop <- TRUE
          } 
        }
      }
      
      # No more data pts to fit
      else if (test_pval <= 0.05 & test_coeff < 0 & e==nrow(datasub)) { # meets threshold but no remaining points to extend
        end <- e
        sim_est <- savefit(start, end, datasub, sim_est)
        stop <- TRUE
      }
    }
  }
}
```

### Remove "essentially perfect fits"

I can't find any guidance on how to determine whether or not a fit is "perfect" but I know that the residuals are essentially equal to 0 for perfect fits. I'll use 0.0001 as a filtering threshold for removal.

```{r}
# Remove "perfect" fits as precaution
sim_est <- filter(sim_est, residuals >= 0.0001)

dim(sim_est)
```

### Select best fit for each time series

* Lowest p-value of the slope

```{r}
# Choose lowest p-value window
sim_lowest_pvals <- sim_est %>% 
  group_by(simulation) %>% 
  summarize(pval = min(pval)) %>% 
  ungroup()

# Filter chosen estimates
sim_best <- sim_est %>% 
  semi_join(sim_lowest_pvals)

dim(sim_best)
```

False positive rates:

```{r}
# False positives
a <- nrow(sim_best[sim_best$pval <= 0.05,])
b <- nrow(sim_best[sim_best$pval <= 0.025,])
c <- nrow(sim_best[sim_best$pval <= 0.01,])
d <- nrow(sim_best[sim_best$pval <= 0.005,])
e <- nrow(sim_best[sim_best$pval <= 0.001,])
f <- nrow(sim_best[sim_best$pval <= 0.0005,])

false_pos <- data.frame(c(0.05, 0.025, 0.01, 0.005, 0.001, 0.0005), c(a,b,c,d,e,f))
colnames(false_pos)=c("pvalue","false")

ggplot(false_pos, aes(x=pvalue, y=false)) +
  geom_point() +
  geom_smooth(method="lm", linetype=2) +
  labs(title="Relationship between p-value and number of false positives", x="P-value", y="False positives") +
  theme_test()
```

Find 10%, 5%, 2.5%, 1% false positive allowance p-value thresholds

* Using linear model to predict

```{r}
falsepos_lm <- lm(false ~ pvalue, data=false_pos)
falsepos_lm
```

To reflect growth estimates, filtering at 5% false positive rate:

```{r}
## 5% false positive allowance
false5_pval <- (50 - intercept)/slope

# ~5% false positives
death_falsepos5 <- subset(death_best, pval <= false5_pval)
nrow(death_falsepos5)
```

# Calculate growth metrics

### Rate

Need to think about specific growth rate equivalent for death. Formula?

```{r}
death_final <- death_falsepos5 %>% 
  select(everything(), rate=coeff)
```


### Start and end day, change in relational abundance

```{r}
# Convert start and end to actual day
death_x <- data.frame()
for (l in as.character(unique(death_final$Label))) {
  # Isolate timeseries
  norm_label <- norm_ln %>% 
    filter(Label==l) %>% 
    arrange(Day)
  death_label <- filter(death_final, Label==l)
  start <- death_label$start
  end <- death_label$end
  # Start and end day of death
  start_day <- norm_label[start,]$Day
  end_day <- norm_label[end,]$Day
  # Starting and ending relational abundance
  start_abund <- norm_label[start,]$norm_abund
  end_abund <- norm_label[end,]$norm_abund
  change_abund <- end_abund - start_abund
  # Save output
  this_row <- bind_cols(Label = as.character(death_label$Label), Soil = as.character(death_label$Soil),
                        Amendment = as.character(death_label$Amendment), Replicate = death_label$Replicate,
                        ASV = as.character(death_label$ASV),
                        rate = death_label$rate,
                        start_pt = death_label$start, end_pt = death_label$end,
                        start_day = start_day, end_day = end_day, 
                        start_abund = start_abund, end_abund = end_abund, change_abund = change_abund)
  death_x <- bind_rows(death_x, this_row)
}
```
