{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Pipeline to Process Raw Sequences into Phyloseq Object with DADA2 ###\n",
    "* Prep for Import to QIIME2  (Combine two index files)\n",
    "* Import to QIIME2\n",
    "* Demultiplex\n",
    "* Denoise and Merge\n",
    "* Prepare OTU Tables and Rep Sequences  *(Note: sample names starting with a digit will break this step)*\n",
    "* Classify Seqs\n",
    "\n",
    "*100% Appropriated from the \"Atacama Desert Tutorial\" for QIIME2*\n",
    "\n",
    "### Pipeline can handle both 16S rRNA gene and ITS sequences (in theory)####\n",
    "* Tested on 515f and 806r\n",
    "* Tested on ITS1\n",
    "\n",
    "### Commands to Install Dependencies ####\n",
    "##### || QIIME2 ||\n",
    "** Note: QIIME2 is still actively in development, and I've noticed frequent new releases. Check for the most up-to-date conda install file <https://docs.qiime2.org/2017.11/install/native/#install-qiime-2-within-a-conda-environment>\n",
    "\n",
    "* wget https://data.qiime2.org/distro/core/qiime2-2018.2-py35-linux-conda.yml\n",
    "* conda env create -n qiime2-2018.2 --file qiime2-2018.2-py35-linux-conda.yml\n",
    "* source activate qiime2-pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### || Copyrighter rrn Database ||\n",
    "* The script will automatically install the curated GreenGenes rrn attribute database\n",
    "* https://github.com/fangly/AmpliCopyrighter\n",
    "\n",
    "##### || rpy2 (don't use conda version) ||\n",
    "* pip install rpy2  \n",
    "\n",
    "##### || phyloseq ||\n",
    "* conda install -c r r-igraph \n",
    "* Rscript -e \"source('http://bioconductor.org/biocLite.R');biocLite('phyloseq')\" \n",
    "\n",
    "##### || R packages ||\n",
    "* ape   (natively installed in conda environment)\n",
    "\n",
    "\n",
    "### Citations ###\n",
    "* Caporaso, J. G., Kuczynski, J., Stombaugh, J., Bittinger, K., Bushman, F. D., Costello, E. K., *et al.* (2010). QIIME allows analysis of high-throughput community sequencing data. Nature methods, 7(5), 335-336.\n",
    "\n",
    "\n",
    "* McMurdie and Holmes (2013) phyloseq: An R Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data. PLoS ONE. 8(4):e61217\n",
    "\n",
    "\n",
    "* Paradis E., Claude J. & Strimmer K. 2004. APE: analyses of phylogenetics and evolution in R language. Bioinformatics 20: 289-290.\n",
    "\n",
    "\n",
    "* Angly, F. E., Dennis, P. G., Skarshewski, A., Vanwonterghem, I., Hugenholtz, P., & Tyson, G. W. (2014). CopyRighter: a rapid tool for improving the accuracy of microbial community profiles through lineage-specific gene copy number correction. Microbiome, 2(1), 11.\n",
    "\n",
    "###### Last Modified by R. Wilhelm on October 12th, 2017 ######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, numpy as np\n",
    "\n",
    "# Provide the directory for your index and read files (you can do multiple independently in one go)\n",
    "growthrate = '/home/cassi/growthrate2018/'\n",
    "# CW: this is the directory where I want to put my outputs from the pipeline\n",
    "\n",
    "seqdir = '/home/backup_files/raw_reads/growthrate.cassi.2018/'\n",
    "# CW: this is the directory where my raw files live\n",
    "\n",
    "# Prepare an object with the name of the library, the name of the directory object (created above), and the metadatafile name\n",
    "#datasets = [['name',directory1,'metadata1','domain of life',raw reads directory],['name',directory2,'metadata2','domain of life']]\n",
    "datasets = [['growthrate',growthrate,'growthrate_metadata.tsv','bacteria',seqdir]]\n",
    "\n",
    "# Ensure your reads files are named accordingly (or modify to suit your needs)\n",
    "readFile1 = '88202_C56H8_GR_Library_S1_R1_001.fastq.gz'\n",
    "readFile2 = '88202_C56H8_GR_Library_S1_R2_001.fastq.gz'\n",
    "indexFile1 = '88202_C56H8_GR_Library_S1_I1_001.fastq.gz'\n",
    "indexFile2 = '88202_C56H8_GR_Library_S1_I2_001.fastq.gz'\n",
    "\n",
    "# Set # of Processors to Use\n",
    "processors = \"20\"\n",
    "# CW: there are 40 processers total, check with others to see how many are being used, affects speed of pipeline\n",
    "\n",
    "# Classification Database to Use \n",
    "# options: \"Silva\" [default] | \"GreenGenes\" \n",
    "db = \"Silva\"\n",
    "# CW: use Silva for bacteria, need UNITE or ITS1db databases for fungi\n",
    "\n",
    "## Enter Minimum Support for Keeping QIIME Classification\n",
    "# Note: Classifications that do not meet this criteria will simply be retained, but labeled 'putative'\n",
    "min_support = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Concatenate Barcodes for QIIME2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: QIIME takes a single barcode file. The command 'extract_barcodes.py' concatenates the forward and reverse read barcode and attributes it to a single read.\n",
    "\n",
    "# See http://qiime.org/tutorials/processing_illumina_data.html\n",
    "\n",
    "# Extracts barcodes from index files, outputs into home directory for project\n",
    "\n",
    "for dataset in datasets:\n",
    "    directory = dataset[1]\n",
    "    rawdir = dataset[4]\n",
    "    index1 = rawdir+indexFile1\n",
    "    index2 = rawdir+indexFile2\n",
    "    \n",
    "    # Run extract_barcodes to merge the two index files\n",
    "    !python2 /opt/anaconda2/bin/extract_barcodes.py --input_type barcode_paired_end -f $index1 -r $index2 --bc1_len 8 --bc2_len 8 -o $directory/output\n",
    "\n",
    "    # QIIME2 import requires a directory containing files names: forward.fastq.gz, reverse.fastq.gz and barcodes.fastq.gz \n",
    "    #links directories instead of copying\n",
    "    !ln -s $rawdir$readFile1 $directory/output/forward.fastq.gz\n",
    "    !ln -s $rawdir$readFile2 $directory/output/reverse.fastq.gz\n",
    "    \n",
    "    # linking raw data directory with output folder in pushpull2018\n",
    "    \n",
    "    # Gzip the barcodes files (apparently necessary)\n",
    "    !pigz -p 5 $directory/output/barcodes.fastq\n",
    "\n",
    "    # Removed orphaned reads files (not needed)\n",
    "    !rm $directory/output/reads?.fastq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Import into QIIME2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4d3557c7f512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     os.system(' '.join([\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime tools import\",\n",
    "        \"--type EMPPairedEndSequences\",\n",
    "        \"--input-path \"+directory+\"output/\",\n",
    "        \"--output-path \"+directory+\"output/\"+name+\".qza\"\n",
    "    ]))\n",
    "    \n",
    "    # This more direct command is broken by the fact QIIME uses multiple dashes in their arguments (is my theory)\n",
    "    #!qiime tools import --type EMPPairedEndSequences --input-path $directory/output --output-path $directory/output/$name.qza\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Demultiplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "## Note: The barcode you supply to QIIME is now a concatenation of your forward and reverse barcode.\n",
    "# Your 'forward' barcode is actually the reverse complement of your reverse barcode and the 'reverse' is your forward barcode. The file 'primers.complete.csv' provides this information corresponding to the Buckley Lab 'primer number'\n",
    "# This quirk could be corrected in how different sequencing facilities pre-process the output from the sequencer\n",
    "\n",
    "##\n",
    "## SLOW STEP (~ 2 - 4 hrs)\n",
    "##\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    metadata = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux emp-paired\",\n",
    "        \"--m-barcodes-file \"+directory+metadata,\n",
    "        \"--m-barcodes-column BarcodeSequence\",\n",
    "        \"--i-seqs \"+directory+\"output/\"+name+\".qza\",\n",
    "        \"--o-per-sample-sequences \"+directory+\"output/\"+name+\".demux\"\n",
    "    ]))\n",
    "    \n",
    "    # This more direct command is broken by the fact QIIME uses multiple dashes in their arguments (is my theory)\n",
    "    #!qiime demux emp-paired --m-barcodes-file $directory/$metadata --m-barcodes-category BarcodeSequence --i-seqs $directory/output/$name.qza --o-per-sample-sequences $directory/output/$name.demux\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"demultiplex finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Visualize Quality Scores and Determine Trimming Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The script will now wait for you to input trimming parameters in the next cell. You will need to take the .qzv files for each library and visualize them at <https://view.qiime2.org>. This is hopefully temporary, while QIIME2 developers improve on q2view.\n",
      "\n",
      "[ENTER ANYTHING. THIS IS ONLY MEANT TO PAUSE THE PIPELINE]anything\n",
      "\n",
      "The script is now proceeding. Stay tuned to make sure trimming works.\n"
     ]
    }
   ],
   "source": [
    "## Based on the Graph Produced using the Following Command enter the trim and truncate values. Trim refers to the start of a sequence and truncate the total length (i.e. number of bases to remove from end)\n",
    "\n",
    "# The example in the Atacam Desert Tutorial trims 13 bp from the start of each read and does not remove any bases from the end of the 150 bp reads:\n",
    "#  --p-trim-left-f 13 \\  \n",
    "#  --p-trim-left-r 13 \\\n",
    "#  --p-trunc-len-f 150 \\\n",
    "#  --p-trunc-len-r 150\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux summarize\",\n",
    "        \"--i-data \"+directory+\"/output/\"+name+\".demux.qza\",\n",
    "        \"--o-visualization \"+directory+\"/output/\"+name+\".demux.QC.summary.qzv\"\n",
    "    ]))\n",
    "    \n",
    "    ## Take the output from this command and drop it into:\n",
    "    #https://view.qiime2.org\n",
    "\n",
    "wait_for_user = input(\"The script will now wait for you to input trimming parameters in the next cell. You will need to take the .qzv files for each library and visualize them at <https://view.qiime2.org>. This is hopefully temporary, while QIIME2 developers improve on q2view.\\n\\n[ENTER ANYTHING. THIS IS ONLY MEANT TO PAUSE THE PIPELINE]\")\n",
    "print(\"\\nThe script is now proceeding. Stay tuned to make sure trimming works.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qsv done\n"
     ]
    }
   ],
   "source": [
    "print(\"qsv done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Trimming Parameters | USER INPUT REQUIRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User Input Required\n",
    "trim_dict = {}\n",
    "\n",
    "## Input your trimming parameters into a python dictionary for all libraries\n",
    "#trim_dict[\"LibraryName1\"] = [trim_forward, truncate_forward, trim_reverse, truncate_reverse]\n",
    "#trim_dict[\"LibraryName2\"] = [trim_forward, truncate_forward, trim_reverse, truncate_reverse]\n",
    "\n",
    "## Example\n",
    "trim_dict[\"growthrate\"] = [5, 240, 5, 200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Trim, Denoise and Join (aka 'Merge') Reads Using DADA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## SLOW STEP (~ 6 - 8 hrs, IF multithreading is used)\n",
    "##\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime dada2 denoise-paired\",\n",
    "        \"--i-demultiplexed-seqs \"+directory+\"/output/\"+name+\".demux.qza\",\n",
    "        \"--o-table \"+directory+\"/output/\"+name+\".table\",\n",
    "        \"--o-representative-sequences \"+directory+\"/output/\"+name+\".rep.seqs.final\",\n",
    "        \"--p-trim-left-f \"+str(trim_dict[name][0]),\n",
    "        \"--p-trim-left-r \"+str(trim_dict[name][2]),\n",
    "        \"--p-trunc-len-f \"+str(trim_dict[name][1]),\n",
    "        \"--p-trunc-len-r \"+str(trim_dict[name][3]),\n",
    "        \"--p-n-threads\",\n",
    "        str(processors)\n",
    "    ]))\n",
    "    \n",
    "    # CW: added str() around processors so that function will work, os.system needs a string passed to it because .join takes strings\n",
    "    # .join concatenating strings into longer string, ' ' using space as delimiter\n",
    "    # os.system puts the string in the shell and runs the string as a qiime command\n",
    "    # qiime runs dada2 on the big string you made, using trim parameters you defined above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Create Summary of OTUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    metadata = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table summarize\",\n",
    "        \"--i-table \"+directory+\"/output/\"+name+\".table.qza\",\n",
    "        \"--o-visualization \"+directory+\"/output/\"+name+\".table.qzv\",\n",
    "        \"--m-sample-metadata-file \"+directory+metadata\n",
    "    ]))\n",
    "\n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table tabulate-seqs\",\n",
    "        \"--i-data \"+directory+\"/output/\"+name+\".rep.seqs.final.qza\",\n",
    "        \"--o-visualization \"+directory+\"/output/\"+name+\".rep.seqs.final.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Make Phylogenetic Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    metadata = dataset[2]\n",
    "    domain = dataset[3]\n",
    "\n",
    "    if domain != \"fungi\":\n",
    "        # Generate Alignment with MAFFT\n",
    "        os.system(' '.join([\n",
    "            \"qiime alignment mafft\",\n",
    "            \"--i-sequences \"+directory+\"/output/\"+name+\".rep.seqs.final.qza\",\n",
    "            \"--o-alignment \"+directory+\"/output/\"+name+\".rep.seqs.aligned.qza\",\n",
    "            \"--p-n-threads\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "        # CW: changed processors into a string again\n",
    "\n",
    "        # Mask Hypervariable parts of Alignment\n",
    "        os.system(' '.join([\n",
    "            \"qiime alignment mask\",\n",
    "            \"--i-alignment \"+directory+\"/output/\"+name+\".rep.seqs.aligned.qza\",\n",
    "            \"--o-masked-alignment \"+directory+\"/output/\"+name+\".rep.seqs.aligned.masked.qza\"\n",
    "        ])) \n",
    "\n",
    "        # Generate Tree with FastTree\n",
    "        os.system(' '.join([\n",
    "            \"qiime phylogeny fasttree\",\n",
    "            \"--i-alignment \"+directory+\"/output/\"+name+\".rep.seqs.aligned.masked.qza\",\n",
    "            \"--o-tree \"+directory+\"/output/\"+name+\".rep.seqs.tree.unrooted.qza\",\n",
    "            \"--p-n-threads\",\n",
    "            processors\n",
    "        ])) \n",
    "\n",
    "        # Root Tree\n",
    "        os.system(' '.join([\n",
    "            \"qiime phylogeny midpoint-root\",\n",
    "            \"--i-tree \"+directory+\"/output/\"+name+\".rep.seqs.tree.unrooted.qza\",\n",
    "            \"--o-rooted-tree \"+directory+\"/output/\"+name+\".rep.seqs.tree.final.qza\"\n",
    "        ])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Classify Seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example of creating Silva Classifier DB  (very slow: use a pre-built one if possible)\n",
    "\n",
    "#### For Silva, remove all the alignment information (unsure of the impact of keeping it) using the following python code:\n",
    "```python \n",
    "import re\n",
    "from Bio import SeqIO\n",
    "\n",
    "output = open(\"silva.nr_v128.fasta\", \"w\")\n",
    "\n",
    "for record in SeqIO.parse(open(\"silva.nr_v128.align\", \"rU\"), \"fasta\") :\n",
    "    seq = str(record.seq)\n",
    "    seq = re.sub(\"\\.|-\",\"\",seq)  # Remove \".\" and \"-\"\n",
    "\n",
    "    output.write(\">\"+record.id+\"\\n\"+seq+\"\\n\")```\n",
    "\n",
    "#### Import fasta sequence file and taxonomy file as .qza\n",
    "```bash\n",
    "qiime tools import\n",
    "  --type 'FeatureData[Sequence]'\n",
    "  --input-path silva.nr_v128.fasta\n",
    "  --output-path silva.nr_v128.qza\n",
    "\n",
    "qiime tools import \n",
    "  --type 'FeatureData[Taxonomy]' \n",
    "  --source-format HeaderlessTSVTaxonomyFormat \n",
    "  --input-path silva.nr_v128.tax \n",
    "  --output-path silva.nr_v128.taxonomy.qza```\n",
    "\n",
    "#### Run QIIME2 'fit-classifier-naive-bayes'\n",
    "```bash\n",
    "qiime feature-classifier fit-classifier-naive-bayes \n",
    "  --i-reference-reads silva.nr_v128.qza \n",
    "  --i-reference-taxonomy silva.nr_v128.taxonomy.qza \n",
    "  --o-classifier silva.nr_v128.nb.classifier.qza```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Different QIIME2 versions can conflict with previously donwloaded databases. This section might have to be updated.\n",
    "try:\n",
    "    if db == \"GreenGenes\":\n",
    "        classification_db = \"/home/db/GreenGenes/qiime2_13.8.99_515.806_nb.classifier.qza\"\n",
    "    else:\n",
    "        classification_db = \"/home/db/Silva/silva.nr_v128.nb.classifier.qza\"\n",
    "        \n",
    "except:\n",
    "        classification_db = \"/home/db/Silva/silva.nr_v128.nb.classifier.qza\"\n",
    "        \n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    metadata = dataset[2]\n",
    "    domain = dataset[3]\n",
    "\n",
    "    # Classify\n",
    "    if domain == 'bacteria':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier\",\n",
    "            classification_db,\n",
    "            \"--i-reads \"+directory+\"/output/\"+name+\".rep.seqs.final.qza\",\n",
    "            \"--o-classification \"+directory+\"/output/\"+name+\".taxonomy.final.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            processors\n",
    "        ]))\n",
    "\n",
    "    if domain == 'fungi':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier /home/db/UNITE/qiime2_unite_ver7.99_20.11.2016_classifier.qza\",\n",
    "            \"--i-reads \"+directory+\"/output/\"+name+\".rep.seqs.final.qza\",\n",
    "            \"--o-classification \"+directory+\"/output/\"+name+\".taxonomy.final.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            processors\n",
    "        ]))\n",
    "\n",
    "    # Output Summary\n",
    "    os.system(' '.join([\n",
    "        \"qiime metadata tabulate\",\n",
    "        \"--m-input-file \"+directory+\"/output/\"+name+\".taxonomy.final.qza\",\n",
    "        \"--o-visualization \"+directory+\"/output/\"+name+\".taxonomy.final.summary.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Prepare Data for Import to Phyloseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make Function to Re-Format Taxonomy File to Contain Full Column Information \n",
    "# and factor in the certain of the taxonomic assignment\n",
    "\n",
    "def format_taxonomy(tax_file, classification_db, min_support):\n",
    "    output = open(re.sub(\".tsv\",\".fixed.tsv\",tax_file), \"w\")\n",
    "\n",
    " \n",
    "    # Silva db lacks species classifications\n",
    "    if classification_db == \"GreenGenes\":\n",
    "        full_rank_length = 7\n",
    "        output.write(\"\\t\".join([\"OTU\",\"Domain\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\",\"Species\"])+\"\\n\")\n",
    "    else:\n",
    "        full_rank_length = 6  \n",
    "        output.write(\"\\t\".join([\"OTU\",\"Domain\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\"])+\"\\n\")\n",
    "        \n",
    "    with open(tax_file, \"r\") as f:\n",
    "        next(f) #skip header\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = line.split(\"\\t\")\n",
    "\n",
    "            read_id = line[0]\n",
    "            tax_string = line[1]\n",
    "\n",
    "            ## Remove All Underscore Garbage (I need aesthetics)\n",
    "            if classification_db == \"GreenGenes\":\n",
    "                tax_string = re.sub(\"k__|p__|c__|o__|f__|g__|s__\",\"\",tax_string)\n",
    "            else:\n",
    "                tax_string = re.sub(\"_cl|_or|_fa|_ge\",\"\",tax_string)\n",
    "            \n",
    "            # Split full rank into ranks\n",
    "            full_rank = tax_string.split(\";\")\n",
    "          \n",
    "            # Getting trailing empty tab in Silva\n",
    "            if full_rank[len(full_rank)-1] == \"\":\n",
    "                    full_rank = full_rank[:-1]\n",
    "                    \n",
    "            ## Identify the Lowest Classified Taxonomic Rank\n",
    "            # Account for cases when a taxonomic rank contains an empty space (common in GreenGenes output)\n",
    "            last_classified = full_rank[len(full_rank)-1]            \n",
    "\n",
    "            count = 1\n",
    "            while last_classified == \" \":\n",
    "                last_classified = full_rank[len(full_rank)-count]\n",
    "                count = count + 1\n",
    "\n",
    "            # Annotate the last classified as 'putative' if it does not meet the minimum support criteria\n",
    "            # Older versions of this script contain code to designate all taxonomic ranks as 'putative' in this case, but \n",
    "            # this seems conservative\n",
    "            if float(line[2]) < float(min_support):\n",
    "                    full_rank[full_rank.index(last_classified)] = \"putative \"+last_classified\n",
    "                    last_classified = \"putative \"+last_classified\n",
    "                    \n",
    "            # Add in columns containing unclassified taxonomic information\n",
    "            try: # In Silva, many classifications are a single entry (which breaks from the reliance on lists for full_rank.index)\n",
    "                for n in range(full_rank.index(last_classified)+1, full_rank_length, 1):               \n",
    "                    try:\n",
    "                        full_rank[n] = \"unclassified \"+last_classified\n",
    "                    except:\n",
    "                        full_rank.append(\"unclassified \"+last_classified)\n",
    "            except:\n",
    "                for n in range(0, full_rank_length, 1):               \n",
    "                    try:\n",
    "                        full_rank[n] = \"unclassified \"+last_classified\n",
    "                    except:\n",
    "                        full_rank.append(\"unclassified \"+last_classified)\n",
    "                    \n",
    "            # Clean-up the trailing whitespace introduced in Silva classification \n",
    "            if classification_db == \"Silva\":\n",
    "                full_rank = [x.strip(' ') for x in full_rank]\n",
    "\n",
    "            # Write Taxonomy to File\n",
    "            output.write(read_id+\"\\t\"+'\\t'.join(full_rank)+\"\\n\")\n",
    "            \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## Export from QIIME2\n",
    "\n",
    "# CW: had to remove first / in front of every output\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    metadata = dataset[2]\n",
    "    domain = dataset[3]\n",
    "\n",
    "    ## Final Output Names\n",
    "    fasta_file = directory+\"/output/\"+name+\".rep.seqs.final.fasta\"\n",
    "    tree_file = directory+\"/output/\"+name+\".tree.final.nwk\"\n",
    "    tax_file = directory+\"/output/\"+name+\".taxonomy.final.tsv\"\n",
    "    count_table = directory+\"/output/\"+name+\".counts.final.biom\"\n",
    "\n",
    "    # Export Classifications\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        directory+\"/output/\"+name+\".taxonomy.final.qza\",\n",
    "        \"--output-dir \"+directory+\"/output/\"\n",
    "    ]))\n",
    "    \n",
    "    # Reformat Classifications to meet phyloseq format   \n",
    "    format_taxonomy(directory+\"/output/taxonomy.tsv\", db, min_support)\n",
    "\n",
    "    # Export SV Table\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        directory+\"/output/\"+name+\".table.qza\",\n",
    "        \"--output-dir \"+directory+\"/output/\"\n",
    "    ]))\n",
    "\n",
    "    # Export SV Sequences\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        directory+\"/output/\"+name+\".rep.seqs.final.qza\",\n",
    "        \"--output-dir \"+directory+\"/output/\"\n",
    "    ]))\n",
    "    \n",
    "    # Export Tree\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        directory+\"/output/\"+name+\".rep.seqs.tree.final.qza\",\n",
    "        \"--output-dir \"+directory+\"/output/\"\n",
    "    ]))\n",
    "    \n",
    "    # Rename Exported Files\n",
    "    %mv $directory/output/dna-sequences.fasta $fasta_file\n",
    "    %mv $directory/output/feature-table.biom $count_table\n",
    "    %mv $directory/output/taxonomy.fixed.tsv $tax_file\n",
    "    \n",
    "    #if domain == \"bacteria\":\n",
    "        #%mv $directory/output/tree.nwk $tree_file\n",
    "        \n",
    "    # CW: above broke the code somehow, moved to new cell and worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv $directory/output/tree.nwk $tree_file"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
