---
title: "SFA2 - Growth estimations"
author: "Cassandra Wattenburger"
date: "9/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "show")
knitr::opts_chunk$set(message = FALSE)
```

```{r}
# Clear working directory, load in packages, generate package info
rm(list=ls())

library("tidyverse")
library("trelliscopejs")

sessionInfo()
```

# Import data

* Normalized count data
* Taxonomy table
* Metadata

See 02.5_SFA2_preprocess_redos.Rmd.

```{r}
# Count data
norm <- readRDS("../data_intermediate/SFA2_count_normalized.rds") %>% 
  pivot_longer(-SampleID, names_to="ASV", values_to="norm_abund") %>% 
  mutate(SampleID = as.double(gsub("sa", "", SampleID)))

# Taxonomy
tax <- readRDS("../data_intermediate/SFA2_tax_normalized.rds")

# Metadata
meta <- read_tsv("../data_amplicon/SFA2_full/SFA2_full_metadata.tsv")

# Merge
norm_prep <- inner_join(norm, tax, by="ASV") %>% 
  inner_join(meta, by="SampleID") %>% 
  select(c(SampleID, Sample, Type:Replicate, Domain:Species, ASV, norm_abund)) %>% 
  mutate(Innoculant = as_factor(Innoculant),
         Replicate = as_factor(Replicate)) %>% 
  mutate(across(Domain:Species, ~as.character(.x), {.col}))

dim(norm_prep)
```

# Prepare data

Remove samples that did not contain internal standard

* These are now Inf because previous script tried to divide by 0

```{r}
norm_prep <- norm_prep[!is.infinite(norm_prep$norm_abund),]

dim(norm_prep)
```

Remove samples that were not part of the growth rate measurement

```{r}
norm_prep <- filter(norm_prep, Type=="Growth")

dim(norm_prep)
```

Remove non-present taxa

```{r}
norm_prep <- norm_prep %>% 
  filter(norm_abund != 0)

dim(norm_prep)
```

Average across technical replicates

* For better coverage of time series
* All samples are destructively sampled anyway and independent of one another, including replicates

```{r}
norm_prep <- norm_prep %>% 
  group_by(Type, Innoculant, DOC, Day, Domain, Phylum, Class, Order, Family, Genus, Species, ASV) %>% 
  summarize(norm_abund_avg = mean(norm_abund, na.rm = TRUE)) %>% 
  ungroup()

dim(norm_prep)
```

Remove taxa that didn't occur in at least four time points

```{r}
# Identify ASVs that appeared less than 4 times in time series
occurences <- norm_prep %>% 
  group_by(ASV, Type, Innoculant) %>% 
  summarize(occurs = n()) %>% 
  filter(occurs > 3) %>% 
  ungroup()

# Filter
norm_prep <- inner_join(norm_prep, occurences, by=c("ASV", "Innoculant", "Type")) %>% 
  select(everything(), -occurs)

dim(norm_prep)
```

Natural log transform:

```{r}
norm_prep <- norm_prep %>% 
  mutate(ln_norm_abund_avg = log(norm_abund_avg))

dim(norm_prep)
```

Create unique label for each ASV and time series:

```{r}
norm_prep <- norm_prep %>%
  mutate(label = paste0(DOC, Innoculant, "_", ASV))

dim(norm_prep)
```

### Visualize time series

All ASVS:

```{r, eval=FALSE}
# Slow
# Opens in browser
norm_prep %>% 
  filter(Type=="Growth") %>% 
  ggplot(aes(x=Day, y=ln_norm_abund_avg, color=Innoculant)) +
  facet_wrap(~DOC) +
  geom_point() +
  geom_line() +
  facet_trelliscope(~ASV, scales="free_y", path="rmarkdown_files", self_contained=TRUE, height = 400, width = 1600) +
  labs(y="ln ARNIS ratio", x="Day") +
  theme_test()
```

Specific examples:

```{r}
# Function to extract taxonomic info for graphs
extract_tax <- function(df, asv, level) { # Data frame, "ASV", "Taxonomy level"
  temp <- df %>% 
    filter(ASV == asv)
  tax <- unique(temp[[level]])
  return(tax)
  }

# Time series graphing function
graph_timeseries <- function(df, asv) {
  graph <- df %>%
    filter(ASV==asv) %>% 
    ggplot(aes(x=Day, y=ln_norm_abund_avg, color=Innoculant)) +
    facet_wrap(~DOC) +
    geom_point() +
    geom_line() +
    labs(y="ln ARNIS ratio", x="Day", title=paste0(extract_tax(norm_prep, asv, "Phylum"), ", ", 
                                                            extract_tax(norm_prep, asv, "Genus"))) +
    theme_test()
  return(graph)
}
```

```{r}
# Graphs
# Robust growth
graph_timeseries(norm_prep, "fed2377b60ef09790fc532bbbe2b4602")
graph_timeseries(norm_prep, "0134683a521e4c34adf2d515cbbca84d")
graph_timeseries(norm_prep, "1030f18ec1c231043abead5e7c3ac109")
graph_timeseries(norm_prep, "4205b2d577048e1da9d9712f9476cd7b")

# Less robust growth
graph_timeseries(norm_prep, "29be7b1a76d1f575ef3ee8d7cc10f0b1")
graph_timeseries(norm_prep, "30a0483ea3c97871b3c91f489fb13ece")
graph_timeseries(norm_prep, "32102f31db06d8a0411da55c4102a9f9")
graph_timeseries(norm_prep, "438ef58af72cb79f6a9220a0b79e976b")
graph_timeseries(norm_prep, "4d0009900aead37d86f6c0a59ee97de3")
graph_timeseries(norm_prep, "5906b37863a7a985418e910925a659ec")
graph_timeseries(norm_prep, "5ce5182a13292aaa88c100640c03afca")
graph_timeseries(norm_prep, "67e786df4bd7ffe42b17907db62caeee")

# Death
graph_timeseries(norm_prep, "13e479df5562e8e93d620ac363eb9644")
graph_timeseries(norm_prep, "2c574662d10f5b34e3b31cdbb9839ba2")
graph_timeseries(norm_prep, "79fa5edf0f6123d3ea5c2c69b897fac1")

# Low abundance issues/noise?
graph_timeseries(norm_prep, "399eb38d8361026b8f897470becd3f13")
graph_timeseries(norm_prep, "39a1ca18bf7123d31dbc41d7c4f03214")
```

# Estimate growth

Prepare data:

```{r}
# Rename columns to match function reqiurements (this is easier to code)
norm_prepped <- norm_prep %>% 
  select(-Type, -c(Domain:Species), norm_abund_avg, time=Day, abund=ln_norm_abund_avg, label, Innoculant, DOC, ASV) %>% 
  select(Innoculant, DOC, ASV, label, time, abund, norm_abund_avg)
```

```{r}
saveRDS(norm_prepped, file="../data_intermediate/SFA2_norm_prepped.rds")
```

Create growth parameter saving function:

```{r}
# Function: Saves chosen growth estimate from below algorhithm
save_fit <- function(label, start, end, df_sub, output) {
  # Save estimate info
  est <- NULL; coeff <- NULL; residuals <- NULL; pval <- NULL; thisrow <- data.frame() # clear previous
  est <- lm(abund ~ time, data=df_sub[start:end,])
  coeff <- as.numeric(est$coefficients[2])
  residuals <- sum(abs(resid(est)))
  pval <- summary(est)$coefficients[2,4]
  thisrow <- data.frame(label, start, end, coeff, pval, residuals)
  output <- bind_rows(output, thisrow)
  return(output)
}
```

Estimate:

```{r}
# Estimate growth

## Requires a dataframe with columns containing a unique label for each data point, abundance values, time points
## df = data frame containing time series with abundance values (ln transformed), long format
## df must contain columns named:
### label = column with unique identifier for each time series
### abund = column with abundance values at each time point
### time = column with time point values
df <- norm_prepped
growth_estimates <- data.frame()
for (label in as.character(unique(df$label))) {
    
  # Subset one time series using the label
  df_sub <- data.frame()
  df_sub <- df[df$label==label,] 
  stop <- FALSE
    
  # Sliding window
  for (start in 1:(nrow(df_sub) - 2)) {
    stop <- FALSE
    for (end in (start + 2):nrow(df_sub)) {
      if (stop == TRUE) {break}
     
      # Fit linear model to the window
      window_lm <- NULL; window_p <- NULL; window_coeff <- NULL
      window_lm <- lm(abund ~ time, data = df_sub[start:end,])
      window_p <- summary(window_lm)$coefficients[2,4]
      window_coeff <- window_lm$coefficients[2]
  
      # If a suitable fit is found and more time points exist that were not included, try extending the window
      if (window_p <= 0.05 & window_coeff > 0 & end < nrow(df_sub)) {
        for (extend_end in ((end+1):nrow(df_sub))) {
          if (stop == TRUE) {break}
            
          # Fit linear model to previous, non-extended window
          prevwindow_lm <- NULL; prevwindow_p <- NULL; prevwindow_coeff <- NULL 
          prevwindow_lm <- lm(abund ~ time, data = df_sub[start:(extend_end - 1),])
          prevwindow_p <- summary(prevwindow_lm)$coefficients[2,4]
          prevwindow_coeff <- prevwindow_lm$coefficients[2]
            
          # Fit linear model to the extended window
          newwindow_lm <- NULL; newwindow_p <- NULL; newwindow_coeff <- NULL 
          newwindow_lm <- lm(abund ~ time, data = df_sub[start:extend_end,])
          newwindow_p <- summary(newwindow_lm)$coefficients[2,4]
          newwindow_coeff <- newwindow_lm$coefficients[2]
            
          # If see improvement and can add more data, continue extending the window
          if (newwindow_p <= prevwindow_p & newwindow_coeff > 0 & extend_end < nrow(df_sub)) {
            next
          }
            
          # If no improvement, save the previous fit
          else if (newwindow_p > prevwindow_p & prevwindow_coeff > 0 & extend_end < nrow(df_sub)) {
            end <- extend_end - 1
            growth_estimates <- save_fit(label, start, end, df_sub, growth_estimates)
            stop <- TRUE
          }
            
          # If see improvement (or no harm) but no more data points to fit, save the extended fit
          else if (newwindow_p <= prevwindow_p & newwindow_coeff > 0 & extend_end == nrow(df_sub)) {
            end <- extend_end
            growth_estimates <- save_fit(label, start, end, df_sub, growth_estimates)
            stop <- TRUE
          } 
        }
      }
        
      # If no more data available to add to model, save the fit
      else if (window_p <= 0.05 & window_coeff > 0 & end == nrow(df_sub)) {
        growth_estimates <- save_fit(label, start, end, df_sub, growth_estimates)
        stop <- TRUE
      }
    }
  }
}

dim(growth_estimates)
```

### Remove "essentially perfect fits"

I can't find any guidance on how to determine whether or not a fit is "perfect" but I know that the residuals are essentially equal to 0 for perfect fits. I'll use 0.0001 as a filtering threshold for removal.

```{r}
# Remove "perfect" fits as precaution
growth_estimates <- filter(growth_estimates, residuals >= 0.0001)

dim(growth_estimates)
```

### Select best fit for each time series

* Lowest p-value of the slope

```{r}
# Choose lowest p-value window
lowest_pvals <- growth_estimates %>% 
  group_by(label) %>% 
  summarize(pval = min(pval)) %>% 
  ungroup()

dim(lowest_pvals)

# Filter chosen estimates
growth_estimates <- growth_estimates %>% 
  semi_join(lowest_pvals)

dim(growth_estimates)
```

# False positive control

Histogram of quality filtered p-values from actual estimates:

```{r}
hist(growth_estimates$pval, xlab="P-values", main="Histogram of quality filtered p-values")
```

This p-value distribution is anti-conservative based on the trailing right tail (this is a good thing). However, it appears that there must be some proportion of false positives based on how high that right tail is compared to the initial spike at the beginning of the distribution. We must do some form of multiple comparison control.

See: http://varianceexplained.org/statistics/interpreting-pvalue-histogram/

I'm choosing to use a permutation approach where I use my growth estimating algorhithm on randomly generated data with characteristics of the real data. I'll use this false positive information to filter my real estimates. Traditional false positive control methods are far too conservative for my dataset.

### Simulate random data

Completely random data designed to reflect actual data, if we detect "significant" growth rate estimates from this, we must control for that, because the same thing can happen in our actual data.

Information about real dataset to use for simulating random data:
* number of time points
* min and max of abundance

```{r, results="show"}
# Minimum and maximum relational abundances
min_abund <- min(norm_prepped$abund)
max_abund <- max(norm_prepped$abund)
avg_abund <- mean(norm_prepped$abund)
sd_abund <- sd(norm_prepped$abund)

hist(norm_prepped$abund)

# Number of time points
num_tps <- norm_prepped %>% 
  group_by(label) %>% 
  summarize(num_points = n()) %>% 
  ungroup()

min_pts <- min(num_tps$num_points)
max_pts <- max(num_tps$num_points)
avg_pts <- mean(num_tps$num_points)
sd_pts <- sd(num_tps$num_points)

hist(num_tps$num_points)
```

Simulate random time series:

```{r}
set.seed(2021)

# Set time points
tps <- unique(meta$Day)
tps <- na.omit(tps)

# Generate using normal distributions
sim_data <- data.frame()
cont <-  TRUE
counter <- 1
while (cont == TRUE) {
  # Stop loop once 1000 simulations have been created
  if (counter == 1001) {
    cont <-  FALSE
  } 
  else {
    thisrow <- data.frame(); hold <- NULL; rand_abund = NULL; rand_pts = NULL; rand_days = NULL # reset values
    # Generate number of time points
    rand_pts <- ceiling(rnorm(1, mean=avg_pts, sd=sd_pts))
    # Make sure number of tps doesn't fall outside min/max
    if (rand_pts <= min_pts | rand_pts >= max_pts) {
      next
      }
    else {
      # Generate abundance for each point
      rand_abund <- rnorm(rand_pts, mean=avg_abund, sd=sd_abund) 
      # Make sure abundances don't fall outside min/max
      if (min(rand_abund) <= min_abund | max(rand_abund) >= max_abund) {
        next
      }
      else {
        # Assign a day to each point
        rand_day <- sort(sample(tps, size=rand_pts, replace=FALSE))
        hold <- cbind(rep(counter, rand_pts), rand_day, rand_abund)
        sim_data= rbind(sim_data, hold)
        counter <- counter + 1
      }
    }
  }
}
colnames(sim_data)[1] <- "simulation"

# Compare simulation to actual data
# Abundances
hist(sim_data$rand_abund)

# Number of data points
sim_num_points <- sim_data %>% 
  group_by(simulation) %>% 
  summarize(num_pts = n()) %>% 
  ungroup() %>% 
  mutate(num_pts = as.numeric(num_pts))
hist(sim_num_points$num_pts)
```

Save simulated data:

* for reproducibility

```{r, eval=FALSE}
saveRDS(sim_data, file="../data_intermediate/SFA2_simulated_data.rds")
```

I'm pretty happy with this. The randomly generated data seems to roughly share characteristics with the actual data.

View some simulated time series:

```{r}
# Choose randomly
sim_data %>%
  filter(simulation == sample(1:1000, 1)) %>% 
  ggplot(aes(x=rand_day, y=rand_abund)) +
    geom_point() +
    geom_line() +
    theme_test()

sim_data %>%
  filter(simulation == sample(1:1000, 1)) %>% 
  ggplot(aes(x=rand_day, y=rand_abund)) +
    geom_point() +
    geom_line() +
    theme_test()

sim_data %>%
  filter(simulation == sample(1:1000, 1)) %>% 
  ggplot(aes(x=rand_day, y=rand_abund)) +
    geom_point() +
    geom_line() +
    theme_test()

sim_data %>%
  filter(simulation == sample(1:1000, 1)) %>% 
  ggplot(aes(x=rand_day, y=rand_abund)) +
    geom_point() +
    geom_line() +
    theme_test()
```

### Estimate "growth" on simulated data

Same method. I really need to turn this into a function.

```{r}
# Prep simulated data (rename columns)
sim_prepped <- sim_data %>% 
  select(label=simulation, abund=rand_abund, time=rand_day)

# Run algorithinmmmnmnm
df <- sim_prepped
sim_estimates <- data.frame()
for (label in as.character(unique(df$label))) {
    
  # Subset one time series using the label
  df_sub <- data.frame()
  df_sub <- df[df$label==label,] 
  stop <- FALSE
    
  # Sliding window
  for (start in 1:(nrow(df_sub) - 2)) {
    stop <- FALSE
    for (end in (start + 2):nrow(df_sub)) {
      if (stop == TRUE) {break}
     
      # Fit linear model to the window
      window_lm <- NULL; window_p <- NULL; window_coeff <- NULL
      window_lm <- lm(abund ~ time, data = df_sub[start:end,])
      window_p <- summary(window_lm)$coefficients[2,4]
      window_coeff <- window_lm$coefficients[2]
  
      # If a suitable fit is found and more time points exist that were not included, try extending the window
      if (window_p <= 0.05 & window_coeff > 0 & end < nrow(df_sub)) {
        for (extend_end in ((end+1):nrow(df_sub))) {
          if (stop == TRUE) {break}
            
          # Fit linear model to previous, non-extended window
          prevwindow_lm <- NULL; prevwindow_p <- NULL; prevwindow_coeff <- NULL 
          prevwindow_lm <- lm(abund ~ time, data = df_sub[start:(extend_end - 1),])
          prevwindow_p <- summary(prevwindow_lm)$coefficients[2,4]
          prevwindow_coeff <- prevwindow_lm$coefficients[2]
            
          # Fit linear model to the extended window
          newwindow_lm <- NULL; newwindow_p <- NULL; newwindow_coeff <- NULL 
          newwindow_lm <- lm(abund ~ time, data = df_sub[start:extend_end,])
          newwindow_p <- summary(newwindow_lm)$coefficients[2,4]
          newwindow_coeff <- newwindow_lm$coefficients[2]
            
          # If see improvement and can add more data, continue extending the window
          if (newwindow_p <= prevwindow_p & newwindow_coeff > 0 & extend_end < nrow(df_sub)) {
            next
          }
            
          # If no improvement, save the previous fit
          else if (newwindow_p > prevwindow_p & prevwindow_coeff > 0 & extend_end < nrow(df_sub)) {
            end <- extend_end - 1
            sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
            stop <- TRUE
          }
            
          # If see improvement (or no harm) but no more data points to fit, save the extended fit
          else if (newwindow_p <= prevwindow_p & newwindow_coeff > 0 & extend_end == nrow(df_sub)) {
            end <- extend_end
            sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
            stop <- TRUE
          } 
        }
      }
        
      # If no more data available to add to model, save the fit
      else if (window_p <= 0.05 & window_coeff > 0 & end == nrow(df_sub)) {
        sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
        stop <- TRUE
      }
    }
  }
}
dim(sim_estimates)
```

### Remove "essentially perfect fits"

I can't find any guidance on how to determine whether or not a fit is "perfect" but I know that the residuals are essentially equal to 0 for perfect fits. I'll use 0.0001 as a filtering threshold for removal.

```{r}
# Remove "perfect" fits as precaution
sim_estimates <- filter(sim_estimates, residuals >= 0.0001)

dim(sim_estimates)
```

### Select best fit for each time series

* Lowest p-value of the slope

```{r}
# Choose lowest p-value window
sim_lowest_pvals <- sim_estimates %>% 
  group_by(label) %>% 
  summarize(pval = min(pval)) %>% 
  ungroup()

dim(sim_lowest_pvals)

# Filter chosen estimates
sim_estimates <- sim_estimates %>% 
  semi_join(sim_lowest_pvals)

dim(sim_estimates)
```

False positive rates:

```{r}
# False positives
a <- nrow(sim_estimates[sim_estimates$pval <= 0.05,])
b <- nrow(sim_estimates[sim_estimates$pval <= 0.025,])
c <- nrow(sim_estimates[sim_estimates$pval <= 0.01,])
d <- nrow(sim_estimates[sim_estimates$pval <= 0.005,])
e <- nrow(sim_estimates[sim_estimates$pval <= 0.001,])
f <- nrow(sim_estimates[sim_estimates$pval <= 0.0005,])

false_pos <- data.frame(c(0.05, 0.025, 0.01, 0.005, 0.001, 0.0005), c(a,b,c,d,e,f))
colnames(false_pos)=c("pvalue","false")

ggplot(false_pos, aes(x=pvalue, y=false)) +
  geom_point() +
  geom_smooth(method="lm", linetype=2) +
  labs(title="Relationship between p-value and number of false positives", x="P-value", y="False positives") +
  theme_test()
  
```

Find 10%, 5%, 2.5%, 1% false positive allowance p-value thresholds

* Using linear model to predict

```{r}
falsepos_lm <- lm(false ~ pvalue, data=false_pos)
falsepos_lm
```

```{r}
# Extract model coefficients
slope <- as.numeric(falsepos_lm$coefficients[2])
intercept <- as.numeric(falsepos_lm$coefficients[1])

# 10% false positive allowance
false10_pval <- (100 - intercept)/slope

## 5% false positive allowance
false5_pval <- (50 - intercept)/slope

# 2.5% false positive allowance
false2.5_pval <- (25 - intercept)/slope

# 1% false positive allowance
false1_pval <- (10 - intercept)/slope
```

Filter p-values based on cut-offs determined (10%, 5%, 2.5%, 1%):

```{r, restuls="show"}
# ~10% false positives
growth_falsepos10 <- subset(growth_estimates, pval <= false10_pval)
nrow(growth_falsepos10)

# ~5% false positives
growth_falsepos5 <- subset(growth_estimates, pval <= false5_pval)
nrow(growth_falsepos5)

# 2.5%
growth_falsepos2.5 <- subset(growth_estimates, pval <= false2.5_pval)
nrow(growth_falsepos2.5)

# 1%
growth_falsepos1 <- subset(growth_estimates, pval <= false1_pval)
nrow(growth_falsepos1)
```

I'll go with 10% false positive rate because it retains more estimates.

# Calculate growth metrics

### Calculate specific growth rate (k)

Steps:

* Use estimated slope from growth curves to simulate growth over a time interval
* Use simulated abundances to solve for k

Using estimates filtered at 10% false positives.

Formula: k=(log10(b)-log10(B))*2.303/t

Where k is specific growth rate, B is abundance at beginning, b is abundance at end, and t is the time interval.

I found this guide helpful: http://miller-lab.net/MillerLab/protocols/general-bacteriology/calculating-growth-rate/

```{r}
# Generate abundances over time interval based on estimated slope
growth_estimates <- data.frame()
B <- 1 # Start with one bacteria
for (l in growth_falsepos10$label) {
  growth_label <- filter(growth_falsepos10, label==l)
  slope <- growth_label$coeff
  b <- (slope*3)+1 # abundance three days later
  k <- (log10(b)-log10(B))*(2.303/3) # calculate k
  this_row <- cbind(growth_label, k)
  growth_estimates <- rbind(growth_estimates, this_row)
}
```

### Start and end day, change in relational abundance

```{r}
# Convert start and end to actual day
growth_final <- data.frame()
for (l in as.character(unique(growth_estimates$label))) {
  # Isolate timeseries
  norm_label <- norm_prepped %>% 
    filter(label==l) %>% 
    arrange(time)
  growth_label <- filter(growth_estimates, label==l)
  start <- growth_label$start
  end <- growth_label$end
  # Start and end day of growth
  start_day <- norm_label[start,]$time
  end_day <- norm_label[end,]$time
  # Starting and ending relational abundance
  start_abund <- norm_label[start,]$norm_abund_avg
  end_abund <- norm_label[end,]$norm_abund_avg
  change_abund <- end_abund - start_abund
  # Save output
  this_row <- bind_cols(label = as.character(growth_label$label), k = growth_label$k,
                        start_pt = growth_label$start, end_pt = growth_label$end,
                        start_day = start_day, end_day = end_day, 
                        start_abund = start_abund, end_abund = end_abund, change_abund = change_abund)
  growth_final <- bind_rows(growth_final, this_row)
}
```

# Tidy up and save data

Relational abundances:

```{r}
# Labels of estimated taxa
est_labels <- as.character(growth_estimates$label)

# Extract from relational abundance table and clean up
norm_tidy <- norm_prepped %>% 
  filter(label %in% est_labels) %>% 
  select(label, everything(), norm_abund_avg, ln_norm_abund_avg=abund, Day=time) %>% 
  select(label, Innoculant, DOC, ASV, Day, norm_abund_avg, ln_norm_abund_avg)
```

```{r, eval=FALSE}
saveRDS(norm_tidy, file="../data_intermediate/SFA2_norm_growth_estimated.rds")
```

Growth estimates:

```{r}
# Add back metadata
growth_tidy <- growth_final %>% 
  mutate(DOC = if_else(grepl("high", label), "high", "low"),
         Innoculant = gsub("[a-z]+([0-9]+)_.+", "\\1", label),
         ASV = gsub("[a-z]+[0-9]+_(.+)", "\\1", label)) %>% 
  select(label, DOC, Innoculant, ASV, k:change_abund)
```

```{r, eval=FALSE}
saveRDS(growth_tidy, file="../data_intermediate/SFA2_growth_estimates.rds")
```

# Plot growth estimates

```{r}
# For messing with graphs
growth_testing <- head(growth_tidy)

for (l in as.character(growth_tidy$label)) {
  # Subset time series
  growth_label <- filter(growth_tidy, label==l)
  norm_label <- filter(norm_tidy, label==l) %>% 
    arrange(Day)
  # Title information
  asv <- growth_label$ASV
  tax_info <- filter(tax, ASV == asv)
  title <- paste0(tax_info$Phylum, ", ", tax_info$Genus)
  # Graph with estimate
  graph <- ggplot(norm_label, aes(x=Day, y=ln_norm_abund_avg)) +
    geom_point(shape=1, size=2, color="#6F7378") +
    geom_line(color="#6F7378") +
    geom_smooth(method="lm", data=norm_label[growth_label$start_pt:growth_label$end_pt,], linetype=2, color="black") +
    labs(title=title, x="Day", y="ln Relational abundance") +
    theme_test() +
    theme(title = element_text(size=18),
        axis.title = element_text(size=16),
        axis.text = element_text(size=14))
  print(graph)
}
```




